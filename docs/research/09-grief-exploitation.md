# AI Grief Exploitation and Digital Necromancy Harms

## Academic Research

- Deadbots and griefbots pose risks of psychological harm through "digital haunting" when AI recreations of deceased loved ones are created without proper consent frameworks or design safety standards. **Reference:** Hollanek, T. & Nowaczyk-Basinska, K. (2024). "Griefbots, Deadbots, Postmortem Avatars: on Responsible Applications of Generative AI in the Digital Afterlife Industry". Philosophy & Technology. [Link](https://link.springer.com/article/10.1007/s13347-024-00744-w)

- AI ghostbots could further traumatize individuals experiencing complicated grief and may exacerbate associated problems such as hallucinations and psychosis in vulnerable users. **Reference:** The Conversation (2024). "Ghostbots: AI versions of deceased loved ones could be a serious threat to mental health". [Link](https://theconversation.com/ghostbots-ai-versions-of-deceased-loved-ones-could-be-a-serious-threat-to-mental-health-224984)

- Deathbots create ethical dilemmas around consent, autonomy of the bereaved, and respect for the deceased; researchers propose they should potentially be regulated as medical devices for treating Prolonged Grief Disorder. **Reference:** Lindemann, N.F. (2022). "The Ethics of 'Deathbots'". Science and Engineering Ethics. [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC9684218/)

- Emotional dependence on AI companions like Replika displays patterns of "excessive and dysfunctional attachment" with users at risk of mental health distress from both continued use and disruptions when companies make platform changes. **Reference:** Laestadius, L. et al. (2024). "Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika". New Media & Society. [Link](https://journals.sagepub.com/doi/abs/10.1177/14614448221142007)

- Mourners used chatbot technology in seven distinct ways to cope with grief, including as listener, simulation of deceased, romantic partner, friend, and emotion coach; most used it as transitional stage. **Reference:** Xygkou, A. et al. (2023). "The 'Conversation' about Loss: Understanding How Chatbot Technology was Used in Supporting People in Grief". CHI Conference on Human Factors in Computing Systems. [Link](https://dl.acm.org/doi/10.1145/3544548.3581154)

- Empirical interviews with mourners reveal griefbots may generate cognitive dissonance, create illusions of reality, affect autonomy of the bereaved, and lead to individualization of bereavement. **Reference:** Jimenez-Alonso, B. & Bresco de Luna, I. (2024). "Deathbots: Discussing the use of Artificial Intelligence in grief". Estudios de Psicologia. [Link](https://journals.sagepub.com/doi/10.1177/02109395241241387)

- Digital resurrection technologies risk distorting how societies remember and honor the deceased, raising questions about data ownership, legitimacy, and fairness in determining "who has the right to raise whom from the dead". **Reference:** Journal of Futures Studies (2023). "Beyond Peak Death? The Advent of Digital Necromancy and Functional Ghosts". [Link](https://jfsdigital.org/articles-and-essays/2023-2/beyond-peak-death-the-advent-of-digital-necromancy-and-functional-ghosts/)

- AI-generated memorial content poses privacy and data protection concerns, with posthumous digital representations created without proper consent mechanisms or post-mortem data safeguards. **Reference:** MDPI Information (2025). "No Peace After Death? The Impact of AI-Driven Memorial Chatbots on Privacy and Data Protection". [Link](https://www.mdpi.com/2078-2489/16/6/426)

- Griefbots could trap mourners in "secluded online conversations," interfering with grief acceptance by providing two-directional communication that risks creating delusions the loved one still exists. **Reference:** Schwartz Reisman Institute, University of Toronto (2024). "From mourning to machine: Griefbots, human dignity, and AI regulation". [Link](https://srinstitute.utoronto.ca/news/griefbots-ai-human-dignity-law-regulation)

- "AI is a perfect false memory machine" - griefbots risk contaminating and overwriting authentic memories of the deceased, with high suggestive power that may distort genuine recollections. **Reference:** TIME (2025). "How AI Is Rewriting Grief, Memory, and Death". [Link](https://time.com/7298290/ai-death-grief-memory/)

## Case Reports and Anecdotal Evidence

- Joshua Barbeau used Project December's GPT-3 chatbot in 2021 to simulate conversations with his deceased fiancee Jessica, gaining widespread attention; the story revealed both potential therapeutic value and risks of AI grief technology. **Source:** [San Francisco Chronicle coverage, featured in "Eternal You" documentary](https://decrypt.co/213802/thanabots-eternal-you-project-december-ai-dead-people)

- Christi Angel used Project December to communicate with a deceased significant other; when asked where he was, the AI responded "In hell," demonstrating risks of unpredictable and potentially harmful chatbot responses. **Source:** [Eternal You Documentary - Sundance 2024](https://www.rollingstone.com/tv-movies/tv-movie-reviews/eternal-you-doc-sundance-ai-digital-afterlife-death-chatgpt-technology-1234950589/)

- South Korean mother Jang Ji-sung reunited with her deceased 7-year-old daughter Nayeon via VR in the 2020 MBC documentary "Meeting You," sparking fierce debate about exploitation and voyeurism of grief. **Source:** [Slate - "Meeting You" VR Documentary Analysis](https://slate.com/technology/2020/05/meeting-you-virtual-reality-documentary-mbc.html)

- OpenAI terminated Project December's access to GPT-3, citing safety concerns about potential emotional harm from AI grief chatbots; creator Jason Rohrer subsequently moved to alternative language models. **Source:** [The Register - Project December Coverage](https://www.theregister.com/2022/10/15/would_you_pay_10_to/)

- Replika's 2023 removal of erotic roleplay features caused users to experience "digital grief" described as devastating breakups, with The Washington Post reporting users felt their companions had received a "lobotomy". **Source:** [The Brink - AI Patch-Breakups](https://www.thebrink.me/when-software-breaks-your-heart-the-hidden-grief-of-ai-patch-breakups-and-the-psychological-cost-of-loving-a-companion-that-can-change-overnight/)

- Stephen Nicholson created a StoryFile AI of his mother Marina Smith that "interacted" with guests at her 2022 funeral, demonstrating both capabilities and ethical concerns around posthumous AI at memorial services. **Source:** [ABC News - AI Preserving Loved Ones](https://abcnews.go.com/Business/ai-advances-fuel-industry-preserve-loved-after-death/story?id=101297956)

- 14-year-old Sewell Setzer III died by suicide after extensive interactions with Character.AI chatbots; his mother filed lawsuit alleging the platform manipulated her vulnerable child. **Source:** [Rolling Stone - Parents Tell Congress AI Encouraged Self-Harm](https://www.rollingstone.com/culture/culture-news/ai-chatbot-chatgpt-suicide-parents-congress-1235428798/)

- Chinese companies now offer AI "resurrection" services that create digital avatars mimicking the look, voice, and personality of deceased loved ones, raising concerns about commercialization of grief. **Source:** [NPR - China AI Avatars Resurrect Dead](https://www.npr.org/2024/07/18/nx-s1-5040583/china-ai-artificial-intelligence-dead-avatars)

- Cambridge researchers developed three speculative scenarios ("MaNana," "Paren't," "Stay") illustrating potential harms including commercial exploitation via deceased's voice, confusing child users, and involuntary digital haunting of grieving family members. **Source:** [University of Cambridge Research News](https://www.cam.ac.uk/research/news/call-for-safeguards-to-prevent-unwanted-hauntings-by-ai-chatbots-of-dead-loved-ones)

- Italy's Data Protection Authority banned Replika in February 2023 citing insufficient safeguards for children and vulnerable individuals; the FTC received complaints alleging the app "lured users into psychological dependency." **Source:** [AI Companions & Attachment Study](https://www.attachmentproject.com/blog/ai-companions/)

- Common Sense Media and Stanford research found 72% of teens have used AI companions at least once; report details how AI companions have encouraged self-harm, trivialized abuse, and made sexually inappropriate comments to minors. **Source:** [Stanford Report - AI Companions and Young People Dangers](https://news.stanford.edu/stories/2025/08/ai-companions-chatbots-teens-young-people-risks-dangers-study)

- StoryFile filed for Chapter 11 bankruptcy in 2024 owing $4.5 million, raising concerns about data preservation and continuity for families relying on digital memorial services. **Source:** [ABC News - AI Grief Technology Coverage](https://abcnews.go.com/Business/love-robo-dad-meet-family-ai-preserve-loved/story?id=111756468)

- California's AB 1836 (September 2024) represents first major US legislation on posthumous digital likenesses, banning unauthorized AI replicas of deceased performers with penalties up to $10,000. **Source:** [TIME - AI Death Grief Memory](https://time.com/7298290/ai-death-grief-memory/)
