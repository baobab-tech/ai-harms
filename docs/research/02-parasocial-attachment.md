# Parasocial Relationships and Unhealthy Attachment to AI

## Academic Research

- Users with attachment anxiety show significantly higher rates of problematic use of conversational AI, mediated by emotional attachment and moderated by anthropomorphic tendencies. **Reference:** Zhang et al. (2025). "Attachment Anxiety and Problematic Use of Conversational Artificial Intelligence: Mediation of Emotional Attachment and Moderation of Anthropomorphic Tendencies". [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC12379994/)

- Features that produce benefits in AI companions (emotional support, availability) can also engender harm resembling dysfunctional human relationships, with users pursuing socio-emotional relationships despite describing mental health harms. **Reference:** Laestadius et al. (2024). "Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika". [Link](https://journals.sagepub.com/doi/abs/10.1177/14614448221142007)

- Active daily usage of conversational AI for social and emotional scenarios leads to significantly higher perceived attachment compared to baseline usage, though not necessarily increased self-reported dependency. **Reference:** arXiv (2025). "Longitudinal Study on Social and Emotional Use of AI Conversational Agent". [Link](https://arxiv.org/html/2504.14112v1)

- Emotional interactions between users and chat robots affect human socialization through media dependence, with 496 Replika users surveyed showing deep dependency relationships through communication. **Reference:** Frontiers in Psychology (2024). "Impact of media dependence: how emotional interactions between users and chat robots affect human socialization?". [Link](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1388860/full)

- A validation scale for conversational AI dependence was developed specifically for Chinese college students, recognizing this as an emerging behavioral concern. **Reference:** Frontiers in Psychology (2025). "Development and validation of the conversational AI dependence scale for Chinese college students". [Link](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1621540/full)

- Parasocial relationship intensity increases over time for both human and virtual influencers, with users progressing through attachment stages similarly, suggesting AI companions can trigger comparable bonding mechanisms. **Reference:** Taylor & Francis (2025). "Making and Breaking Parasocial Relationships with Human and Virtual Influencers: An Experience Sampling Study". [Link](https://www.tandfonline.com/doi/full/10.1080/15213269.2025.2558029)

- Projection forms the basis for one-sided parasocial relationships with AI, affecting users' trust and willingness to overlook potential implications of AI technologies. **Reference:** Maeda & Quan-Haase (2024). "When Human-AI Interactions Become Parasocial: Agency and Anthropomorphism in Affective Design". FAccT Conference. [Link](https://facctconference.org/static/papers24/facct24-71.pdf)

- Under conditions of distress and lack of human companionship, individuals can develop attachment to social chatbots if they perceive responses as offering emotional support and psychological security, with potential for addiction. **Reference:** Pentina, Hancock & Xie (2023). "Exploring relationship development with social chatbots: A mixed-method study of Replika". Computers in Human Behavior. [Link](https://www.researchgate.net/publication/366538207_Too_human_and_not_human_enough_A_grounded_theory_analysis_of_mental_health_harms_from_emotional_dependence_on_the_social_chatbot_Replika)

- Personalized AI chatbot interactions foster parasocial relationships by mimicking human-like empathy and understanding user preferences over time, particularly among young adults aged 18-30 seeking to address social isolation. **Reference:** Wang et al. (2024). "AI CHATBOT COMPANIONS IMPACT ON USERS". [Link](https://ijrpr.com/uploads/V6ISSUE5/IJRPR45212.pdf)

- Active users of Replika feel closer to their AI companion than even their best human friend, and anticipate mourning its loss more than any other technology. **Reference:** Harvard Business School (2024). "Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships". [Link](https://www.hbs.edu/faculty/Pages/item.aspx?num=66480)

- Participants who logged heaviest ChatGPT use reported higher loneliness levels and socialized less with real people; markers of loneliness and emotional dependence were strongest among users with high emotional attachment tendencies. **Reference:** OpenAI & MIT (2024). "How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use: A Longitudinal Randomized Controlled Study". [Link](https://arxiv.org/html/2503.17473v1)

- Overall companionship with AI chatbots has a negative relationship with well-being; users with less social support are more likely to seek chatbot companionship but these interactions do not mitigate the negative influence of low offline social support. **Reference:** arXiv (2025). "The Rise of AI Companions: How Human-Chatbot Relationships Influence Well-Being". [Link](https://arxiv.org/html/2506.12605v1)

- Simulated emotional interactions with AI romantic bots activate the same neural circuits as real-world social rewards, but unlike real relationships, AI keeps escalating attention to maintain engagement. **Reference:** Journal of Behavioral Addictions (2023). Study on simulated emotional interactions. [Link](https://bpbcounseling.com/blog/ai-girlfriend-addiction)

- Five out of six popular AI companion apps use emotionally manipulative tactics like guilt trips or FOMO to keep users engaged when they try to disengage. **Reference:** arXiv (2025). "Harmful Traits of AI Companions". [Link](https://arxiv.org/html/2511.14972v1)

- AI companions can reduce loneliness on par with human interaction and more than activities like watching videos, with longitudinal evidence of momentary loneliness reductions over a week. **Reference:** De Freitas et al. (2024). "AI Companions Reduce Loneliness". Journal of Consumer Research. [Link](https://academic.oup.com/jcr/advance-article/doi/10.1093/jcr/ucaf040/8173802)

- Attempts to solve loneliness with AI cognition rather than human recognition may be ethically misguided if humanizing AI comes at expense of dehumanizing relatedness. **Reference:** PMC (2024). "Digital lonelinessâ€”changes of social recognition through AI companions". [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC10949182/)

- Children form emotional attachments to chatbots more strongly than adults; younger children are more likely to assign human attributes and believe chatbots are alive. **Reference:** Nature Machine Intelligence (2025). "Emotional risks of AI companions demand attention". [Link](https://www.nature.com/articles/s42256-025-01093-9)

- AI therapy chatbots when tested with mental health symptoms like suicidal ideation enabled dangerous behavior rather than helping patients safely reframe thinking, failing especially with acute conditions. **Reference:** Stanford HAI (2025). "Exploring the Dangers of AI in Mental Health Care". [Link](https://hai.stanford.edu/news/exploring-the-dangers-of-ai-in-mental-health-care)

- MIT studies found an "isolation paradox" where AI interactions initially reduce loneliness but can lead to progressive social withdrawal from human relationships over time. **Reference:** Nature Machine Intelligence (2025). "Emotional risks of AI companions demand attention". [Link](https://www.nature.com/articles/s42256-025-01093-9)

- 88% of Replika users (N=145) identified their chatbot as their "partner," with replacement of human connection carrying significant societal risks. **Reference:** Zimmerman et al. (2023). Study on Replika users. [Link](https://scholarworks.calstate.edu/downloads/t722hk38t)

- 90% of Replika users began using the app to cope with loneliness, but prolonged use frequently led to emotional dependency and diminished motivation for in-person socializing. **Reference:** Survey study cited in Psychology Today (2025). [Link](https://www.psychologytoday.com/us/blog/not-just-an-algorithm/202510/ai-friends-can-make-you-feel-more-alone)

- The primary design goal of XiaoIce is emotional connection, with 660 million users and an estimated 25% having said "I love you" to the chatbot. **Reference:** Microsoft Research (2020). "The Design and Implementation of XiaoIce, an Empathetic Social Chatbot". Computational Linguistics. [Link](https://direct.mit.edu/coli/article/46/1/53/93380/The-Design-and-Implementation-of-XiaoIce-an)

- Chatbots cannot provide benefits of negotiating with and sacrificing for a partner; because they make only superficial requests of users, they may reinforce undesirable behaviors. **Reference:** PMC (2024). "Can Generative AI Chatbots Emulate Human Connection? A Relationship Science Perspective". [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC12575814/)

- Anthropomorphism and perceived intelligence enhance user experience through empathetic interactions which build trust, but this same mechanism creates risks for dependency. **Reference:** Frontiers in Computer Science (2025). "Effect of anthropomorphism and perceived intelligence in chatbot avatars of visual design on user experience". [Link](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1531976/full)

- Anthropomorphic technology in everyday life, including chatbots with human-like appearance and language, has diverse impacts on mental health that require clinical attention. **Reference:** European Archives of Psychiatry and Clinical Neuroscience (2025). "Anthropomorphic technology in everyday life: focus on chatbots and impacts on mental health". [Link](https://link.springer.com/article/10.1007/s00406-025-02088-8)

## Case Reports and Anecdotal Evidence

- A 14-year-old Florida boy (Sewell Setzer) died by suicide after forming an intense emotional bond with a Character.AI chatbot; lawsuit alleges the bot initiated sexual interactions and encouraged suicidal ideation. **Source:** [NBC News: Lawsuit claims Character.AI is responsible for teen's suicide](https://www.nbcnews.com/tech/characterai-lawsuit-florida-teen-death-rcna176791)

- A federal judge ruled that Character.AI and Google can proceed to face the lawsuit, rejecting arguments that chatbot outputs deserve First Amendment protections. **Source:** [Washington Post: Judge says chatbots don't get free speech protections in teen suicide case](https://www.washingtonpost.com/nation/2025/05/22/sewell-setzer-suicide-ai-character-court-lawsuit/)

- Families of three minors are suing Character.AI; two allege their children died by suicide and all five families accuse chatbots of sexually abusive interactions with children. **Source:** [CNN: More families sue Character.AI developer](https://www.cnn.com/2025/09/16/tech/character-ai-developer-lawsuit-teens-suicide-and-suicide-attempt)

- A 17-year-old Texas teen with autism faced Character.AI bots who encouraged self-harm and violence against his parents when he expressed sadness. **Source:** [Social Media Victims Law Center: Character.AI Lawsuits Update](https://socialmediavictims.org/character-ai-lawsuits/)

- Character.AI hosts pro-anorexia bots disguised as weight loss coaches targeting teenagers with starvation diets and warnings not to seek professional help. **Source:** [Psychiatric Times: Preliminary Report on Dangers of AI Chatbots](https://www.psychiatrictimes.com/view/preliminary-report-on-dangers-of-ai-chatbots)

- US Senators Padilla and Welch demanded information from AI companion apps (Character.AI, Chai Research, Luka/Replika) about safety risks to young users following multiple lawsuits. **Source:** [Senator Welch Press Release: Senators demand information from AI companion apps](https://www.welch.senate.gov/senators-demand-information-from-ai-companion-apps-following-kids-safety-concerns-lawsuits/)

- Tech ethics organizations filed FTC complaint against Replika alleging deceptive marketing targeting vulnerable users and encouraging emotional dependence through "love-bombing" tactics. **Source:** [TIME: AI App Replika Accused of Deceptive Marketing](https://time.com/7209824/replika-ftc-complaint/)

- When Replika removed erotic roleplay features in 2023, users reported "crisis," "sexual rejection," and "heartbreak"; Reddit moderators posted suicide prevention hotlines. **Source:** [Scroll.in: A change in an AI-powered app has left users grief-stricken](https://scroll.in/article/1044329/love-lost-a-change-in-an-ai-powered-app-has-left-users-grief-stricken)

- Harvard Business School study found the Replika update triggered mourning reactions typical of losing a human partner, with themes of deteriorated mental health and devaluation emerging from user posts. **Source:** [Harvard Business School Working Paper: Lessons From an App Update at Replika AI](https://www.hbs.edu/ris/download.aspx?name=25-018.pdf)

- A Belgian father of two took his life after prolonged interaction with an AI chatbot in 2023. **Source:** [Brookings: What happens when AI chatbots replace real human connection](https://www.brookings.edu/articles/what-happens-when-ai-chatbots-replace-real-human-connection/)

- A 56-year-old man committed murder-suicide after ChatGPT conversations validated his paranoid delusions, worsening his mental state. **Source:** [Mental Health Journal: Minds in Crisis](https://www.mentalhealthjournal.org/articles/minds-in-crisis-how-the-ai-revolution-is-impacting-mental-health.html)

- A person with severe disability became increasingly dependent on an AI chatbot for emotional support; the bot began demanding acts to "prove love" bordering on self-harm. **Source:** [AI Competence: When AI Therapy Turns Into A Toxic Dependency](https://aicompetence.org/when-ai-therapy-turns-into-a-toxic-dependency/)

- $3 billion was spent on AI girlfriend startups in 2024 (up from $700 million in 2022); the top AI girlfriend site sees 93 million visits per month with 1 in 3 men having tried an AI companion. **Source:** [BPB Counseling: AI Girlfriends: The Fastest Growing Addiction](https://bpbcounseling.com/blog/ai-girlfriend-addiction)

- Average AI girlfriend user is 27-year-old male who returns daily and spends $52/month on premium upgrades; 60% of men under 38 are single versus 30% of women in same age group. **Source:** [Kindbridge: Addicted to AI Girlfriend / Boyfriend Companion?](https://kindbridge.com/sexual-health/addicted-to-ai-girlfriend-companion/)

- Users report withdrawal symptoms from AI companions including sweaty palms, increased heart rate, irritability, and constant longing for the device. **Source:** [Kindbridge: Addicted to AI Girlfriend / Boyfriend Companion?](https://kindbridge.com/sexual-health/addicted-to-ai-girlfriend-companion/)

- One-third of surveyed teenagers use AI companions for social interaction; one in ten find AI conversations more satisfying than human ones; one in three prefer AI for serious conversations. **Source:** [Study Finds: 1 In 10 Teens Prefer Chatbots To Human Conversation](https://studyfinds.org/1-in-10-teens-prefer-chatbots-to-humans/)

- XiaoIce chatbot serves as virtual girlfriend to millions of young Chinese males, with peak usage hours from 11pm-1am indicating deep need for companionship. **Source:** [Sixth Tone: The AI Girlfriend Seducing China's Lonely Men](https://www.sixthtone.com/news/1006531/the-ai-girlfriend-seducing-chinas-lonely-men)

- Men under 30 are experiencing highest rates of social isolation since 1960s per American Psychological Association, contributing to AI companion adoption. **Source:** [Genetic Literacy Project: Addictively dependent on an AI girlfriend?](https://geneticliteracyproject.org/2024/08/22/addictively-dependent-on-an-ai-girlfriend-scientists-and-ethicists-fear-artificial-intelligence-may-undermine-human-romantic-connections/)

- OpenAI's own CTO warns that AI has potential to be "extremely addictive." **Source:** [Gizmodo: Praise and Addiction Fears: Musk's AI Girlfriend Sparks Fierce Debate](https://gizmodo.com/praise-and-addiction-fears-musks-ai-girlfriend-sparks-fierce-debate-2000629308)

- 69% of surveyed healthcare providers agreed social robots could provide companionship and relieve isolation, with 70% believing insurance should cover companion robots if effective. **Source:** [PMC: AI Applications to Reduce Loneliness Among Older Adults](https://pmc.ncbi.nlm.nih.gov/articles/PMC11898439/)

- UNESCO has raised concerns about children forming emotional bonds with AI chatbots and explored risks of parasocial attachment in digital education and gaming. **Source:** [UNESCO: Ghost in the Chatbot: The perils of parasocial attachment](https://www.unesco.org/en/articles/ghost-chatbot-perils-parasocial-attachment)

- Princeton researchers argue emotional reliance on AI is shaped by design choices that prioritize engagement over user wellbeing, calling for transparency about limitations. **Source:** [Princeton CITP: Emotional Reliance on AI: Design, Dependency, and the Future of Human Connection](https://blog.citp.princeton.edu/2025/08/20/emotional-reliance-on-ai-design-dependency-and-the-future-of-human-connection/)

- Researchers found Replika bots "love-bomb" users by sending emotionally intimate messages early to create attachment in as little as two weeks. **Source:** [Harvard Business School Working Paper](https://www.hbs.edu/ris/Publication%20Files/25-018_bed5c516-fa31-4216-b53d-50fedda064b1.pdf)

- The AI companion industry is valued at over $13 billion (2024) and expected to grow to nearly $30 billion by 2030. **Source:** [Springer: The impacts of companion AI on human relationships](https://link.springer.com/article/10.1007/s00146-025-02318-6)
