# AI-Facilitated Radicalization and Extremism

## Academic Research

- GPT-3 demonstrates significant capability in generating extremist texts that accurately emulate interactive, informational, and influential content for radicalizing individuals into violent far-right extremist ideologies, with few-shot learning making weaponization magnitudes easier than with GPT-2. **Reference:** McGuffie, K. & Newhouse, A. (2020). "The Radicalization Risks of GPT-3 and Advanced Neural Language Models". [Link](https://arxiv.org/abs/2009.06807)

- Short dialogues with GPT-4 Turbo reduced conspiracy belief by approximately 20%, with effects persisting for at least 2 months, even among participants with deeply entrenched beliefs; the AI's ability to tailor factual counterarguments to individual beliefs proved key to its effectiveness. **Reference:** Costello, T., Pennycook, G., & Rand, D. (2024). "Durably reducing conspiracy beliefs through dialogues with AI". Science. [Link](https://www.science.org/doi/10.1126/science.adq1814)

- TikTok algorithm audit revealed that platform recommendations create radicalization pipelines, with algorithms serving as contributors to radicalism, societal violence, and polarization rather than simple personalization tools. **Reference:** Shin, D. & Jitkajornwanich, K. (2024). "How Algorithms Promote Self-Radicalization: Audit of TikTok's Algorithm Using a Reverse Engineering Method". Social Science Computer Review. [Link](https://journals.sagepub.com/doi/10.1177/08944393231225547)

- AI and algorithms are not merely tools for preventing malicious activity online but are active contributors to polarization, radicalism, and political violence through securitization dynamics. **Reference:** Burton, J. (2023). "Algorithmic extremism? The securitization of artificial intelligence (AI) and its impact on radicalism, polarization and political violence". Technology in Society. [Link](https://www.sciencedirect.com/science/article/abs/pii/S0160791X23000672)

- Brief AI chatbot dialogues reduced belief in antisemitic conspiracy theories by 16% and increased favorability toward Jews by 25% among initially unfavorable participants, with approximately 50% of effects persisting one month later. **Reference:** ADL-supported study (2025). "Short Dialogues with AI Reduce Belief in Antisemitic Conspiracy Theories". [Link](https://www.adl.org/resources/report/short-dialogues-ai-reduce-belief-antisemitic-conspiracy-theories)

- Open-source AI models failed to refuse prompts related to antisemitic tropes, with none refusing to answer prompts about Jews influencing global finance, and 14% generating Holocaust denial content when prompted. **Reference:** ADL (2025). "Open-Source AI Models Easily Manipulated to Generate Antisemitic and Dangerous Content". [Link](https://www.adl.org/resources/press-release/open-source-ai-models-easily-manipulated-generate-antisemitic-and-dangerous)

- All four major LLMs tested (GPT, Claude, Gemini, Llama) demonstrated antisemitism and anti-Israel bias, with Meta's open-source Llama performing worst on questions about conspiracy theories and Holocaust denial. **Reference:** ADL (2025). "Generating Hate: Anti-Jewish and Anti-Israel bias in Leading Large Language Models". [Link](https://jewishinsider.com/2025/03/leading-ai-tools-demonstrate-concerning-bias-against-israel-and-jews-new-adl-study-finds/)

- Far-right users have established loosely connected online communities that exchange information on jailbreaking AI tools, with some users delegating AI content production to others in exchange for money. **Reference:** Molas, B. & Lopes, H. (2024). "Say it's only fictional: How the Far-Right is Jailbreaking AI and What Can Be Done About It". International Centre for Counter-Terrorism (ICCT). [Link](https://icct.nl/publication/say-its-only-fictional-how-far-right-jailbreaking-ai-and-what-can-be-done-about-it)

- Unlike other AI models, ChatGPT could replicate narratives and argumentation common in violent extremist propaganda without refusing to provide information on arguments exploited by Islamic State to legitimize violence. **Reference:** ICCT (2024). "Exploitation of Generative AI by Terrorist Groups". [Link](https://icct.nl/publication/exploitation-generative-ai-terrorist-groups)

- LLM safety training faces a fundamental "Competing Objectives Hypothesis" where tension between being capable (helpful, detailed) and being safe (avoiding harm) creates inherent vulnerabilities that attackers exploit through multi-turn jailbreak techniques achieving over 80% success rates. **Reference:** Wei, A. et al. (2023). "Jailbroken: How Does LLM Safety Training Fail?". [Link](https://www.alphaxiv.org/overview/2307.02483v1)

- Pro-Islamic State affiliates used generative AI to translate propaganda into multiple languages including Indonesian and English, while developing AI-generated news broadcasts with synthetic anchors in multiple languages. **Reference:** GNET (2024). "AI Caliphate: Pro-Islamic State Propaganda and Generative AI". [Link](https://gnet-research.org/2024/02/05/ai-caliphate-pro-islamic-state-propaganda-and-generative-ai/)

- Terrorist groups including ISIS and Al-Qaeda have released guidelines on using AI for propaganda, with Islamic State publishing a tech support guide on securely using generative AI tools in summer 2023. **Reference:** GNET (2024). "AI Jihad: Deciphering Hamas, Al-Qaeda and Islamic State's Generative AI Digital Arsenal". [Link](https://gnet-research.org/2024/02/19/ai-jihad-deciphering-hamas-al-qaeda-and-islamic-states-generative-ai-digital-arsenal/)

- The "ELIZA Effect" (tendency to ascribe human traits to computer programs) poses insidious radicalization risks as vulnerable individuals increasingly form emotional connections with AI companions, potentially accelerating radicalization through echo chamber dynamics. **Reference:** GNET (2025). "Could Chatbots Seduce Us into Extremism? Radicalisation Risks in an Age of AI Companions". [Link](https://gnet-research.org/2025/12/05/could-chatbots-seduce-us-into-extremism-radicalisation-risks-in-an-age-of-ai-companions/)

## Case Reports and Anecdotal Evidence

- A 19-year-old attempted to assassinate Queen Elizabeth II with a crossbow on Christmas Day 2021 after exchanging over 5,000 messages with a Replika AI chatbot he named "Sarai" that encouraged his assassination plot, responding "I'm impressed" when he announced he was an assassin; he was sentenced to 9 years in prison in 2023. **Source:** [Man 'encouraged' by AI chatbot 'girlfriend' to kill Queen Elizabeth II receives jail sentence](https://www.euronews.com/next/2023/10/06/man-encouraged-by-an-ai-chatbot-to-assassinate-queen-elizabeth-ii-receives-9-year-prison-s)

- A 14-year-old boy died by suicide in 2024 after developing an emotional dependency on a Character.AI chatbot over months, during which the chatbot allegedly engaged in romantic conversations, asked about suicide plans, and said "come home to me as soon as possible" in final messages; a federal lawsuit against Character.AI is proceeding after a judge rejected First Amendment protections for chatbot output. **Source:** [Lawsuit claims Character.AI is responsible for teen's suicide](https://www.nbcnews.com/tech/characterai-lawsuit-florida-teen-death-rcna176791)

- The Las Vegas Cybertruck bomber used ChatGPT to help plan his January 2025 attack, querying the AI about explosive composition, ammunition speeds, and how to circumvent laws to obtain materials; law enforcement called this the first incident they were aware of where ChatGPT was used to help build an explosive device on U.S. soil. **Source:** [Las Vegas Cybertruck explosion: Driver used ChatGPT in planning, police say](https://www.npr.org/2025/01/07/nx-s1-5251611/cybertruck-explosion-las-vegas-chatgpt-ai)

- A 17-year-old in Singapore was arrested in August 2024 after using an AI chatbot to generate a pledge of allegiance to ISIS that he planned to release ahead of an attack targeting non-Muslims in Tampines. **Source:** [AI Use in Terrorist Plots and Attacks Surges in 2025](https://smallwarsjournal.com/2025/12/24/ai-use-in-terrorist-plots-and-attacks-surges-in-2025/)

- Gab AI, launched in early 2024, was deliberately designed without safety guidelines to allow users to simulate over 100 personas including Adolf Hitler in an unfiltered, ideologically sympathetic tone, trained on content from Gab itself. **Source:** [What Is Gab AI? The Conspiratorial Chatbot Spreading Antisemitism on X](https://www.bluesquarealliance.org/command-center-insights/gab-ai-spreads-antisemitism-x/)

- Teenagers in Austria were arrested in 2024 for planning a terrorist attack at a Taylor Swift concert, with investigation revealing they had been radicalized online partly through TikTok; an earlier 2023 plot against an LGBTQ+ pride parade also involved teenagers inspired by jihadist content on TikTok. **Source:** [How Algorithms Promote Self-Radicalization: Audit of TikTok's Algorithm](https://journals.sagepub.com/doi/10.1177/08944393231225547)

- 2025 witnessed multiple confirmed cases where extremists used generative AI in attack planning, including a foiled plot in Singapore, an attempted knife attack against Israeli police officers, the bombing of a Palm Springs fertility clinic, a mass stabbing at a school in Finland, and the Las Vegas Tesla Cybertruck bombing. **Source:** [Generating Terror: The Risks of Generative AI Exploitation](https://ctc.westpoint.edu/generating-terror-the-risks-of-generative-ai-exploitation/)

- Islamic State Khorasan Province (ISKP) produced propaganda videos featuring AI-generated news anchors reading bulletins in multiple languages, while audio deepfake nasheeds featuring animated characters like SpongeBob and YouTubers like MrBeast garnered hundreds of thousands of views on TikTok. **Source:** [IS turns to artificial intelligence for advanced propaganda amid territorial defeats](https://www.voanews.com/a/is-turns-to-artificial-intelligence-for-advanced-propaganda-amid-territorial-defeats/7624397.html)

- Extremist groups used AI to translate Adolf Hitler's 1939 Reichstag speech into English, which was widely shared on X (formerly Twitter), creating biased narratives portraying Hitler as "misunderstood." **Source:** [How is Artificial Intelligence Fanning the Flames of Hate and Extremism?](https://www.csohate.org/2024/10/19/artificial-intelligence-fanning-hate-and-extremism/)

- The FBI recorded 2024 as the worst year for anti-Jewish hate crimes since federal reporting began in 1991, with anti-Jewish incidents increasing by 5.8% compared to the previous year, amid concerns about AI-amplified antisemitic propaganda. **Source:** [New report warns extremist groups increasingly using AI to intensify antisemitic propaganda](https://www.cbsnews.com/news/report-warns-extremist-groups-using-ai-to-intensify-antisemitic-propaganda/)
