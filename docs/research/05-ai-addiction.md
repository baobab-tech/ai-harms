# AI Addiction and Compulsive Use

## Academic Research

### Measurement and Assessment

- Four scales measuring ChatGPT addiction have been developed, including the Problematic ChatGPT Use Scale (PCGUS), Problematic Use of Conversational AI scale (PUCAI-6), and PCUS-11, all framed after substance use disorder criteria. **Reference:** Yu et al. (2024); Hu et al. (2023). [Link](https://link.springer.com/article/10.1007/s11469-025-01509-y)

- The Conversational AI Dependence Scale (CAIDS) was developed with 20 items comprising four dimensions: uncontrollability, withdrawal symptoms, mood modification, and negative impacts, validated among Chinese college students. **Reference:** Frontiers in Psychology (2025). "Development and validation of the conversational AI dependence scale for Chinese college students". [Link](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1621540/full)

- The Problematic AI Chatbot Use (PACU) scale, adapted from the Bergen Social Media Addiction Scale, assesses compulsive chatbot usage with items like "I tried to cut down on the use of AI chatbots but failed". **Reference:** Frontiers in Psychology (2025). "Connecting self-esteem to problematic AI chatbot use". [Link](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1453072/full)

### Longitudinal and Controlled Studies

- A four-week randomized controlled trial (n=981, >300K messages) found that higher daily chatbot usage correlated with higher loneliness, emotional dependence, problematic use, and lower socialization across all modalities and conversation types. **Reference:** OpenAI & MIT Media Lab (2025). "Early methods for studying affective use and emotional well-being on ChatGPT". [Link](https://openai.com/index/affective-use-study/)

- Voice mode chatbot interaction showed mixed effects: better well-being when used briefly, but worse outcomes with prolonged daily use; personal conversations were associated with higher loneliness but lower emotional dependence at moderate usage levels. **Reference:** MIT Media Lab & OpenAI (2025). [Link](https://www.media.mit.edu/posts/openai-mit-research-collaboration-affective-use-and-emotional-wellbeing-in-ChatGPT/)

- A longitudinal study found that users who chatted with ChatGPT the longest tended to be lonelier and got more stressed out over subtle changes in the model's behavior, and were more likely to consider the chatbot a "friend". **Reference:** OpenAI & MIT Media Lab (2025). [Link](https://fortune.com/2025/03/24/chatgpt-making-frequent-users-more-lonely-study-openai-mit-media-lab/)

- A large-scale survey (n=404) of regular companion chatbot users found a small but significant direct correlation between session length and loneliness, with social attraction and neuroticism as moderators. **Reference:** ArXiv (2024). "Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users". [Link](https://arxiv.org/html/2410.21596v1)

### Prevalence and Demographics

- A Taiwanese study of 183 university students found average self-reported ChatGPT addiction was low (M=2.02, SD=1.00), with achievement motivation and ChatGPT self-efficacy having significant negative influence on addiction. **Reference:** Hong & Chen (2024). "Effects of Achievement Motivation and ChatGPT Self-Efficacy on ChatGPT Addiction". [Link](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5064583)

- A government-backed study found that 24.19% of teenagers reported experiencing some level of dependency on AI tools. **Reference:** Mobicip (2024). "Teen AI Addiction: Risks of Chatbots and AI Companions". [Link](https://www.mobicip.com/blog/teen-ai-chatbot-addiction)

- Common Sense Media survey (n=1,060, ages 13-17) found 72% of teens have used AI companion chatbots, 33% have relationships or friendships with them, and 23% trust AI companions "quite a bit" or "completely". **Reference:** Common Sense Media (2025). "Talk, Trust, and Trade-Offs: How and Why Teens Use AI Companions". [Link](https://www.commonsensemedia.org/research/talk-trust-and-trade-offs-how-and-why-teens-use-ai-companions)

- Aura parental monitoring app found 36.4% of users ages 13-17 devoted AI conversations to sexual or romantic role-playing in the past six months, making it the most common topic. **Reference:** Washington Times (2025). "Teens reaching AI companions for sex, report finds". [Link](https://www.washingtontimes.com/news/2025/sep/9/teens-reaching-ai-companions-sex-report-finds/)

### Vulnerable Populations

- Individuals with lower self-esteem are more likely to develop reliance on chatbots, compensating for difficulties in social relations; people with higher social anxiety, loneliness, or tendency toward rumination are particularly vulnerable to problematic chatbot use. **Reference:** Frontiers in Psychology (2025). "Connecting self-esteem to problematic AI chatbot use". [Link](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1453072/full)

- Large-scale studies of Character.AI users show people with smaller social networks are more likely to seek chatbot companionship, yet intensive and emotionally self-disclosing use is consistently linked to lower well-being. **Reference:** MIT Media Lab (2025). "Supportive? Addictive? Abusive? How AI companions affect our mental health". [Link](https://www.media.mit.edu/articles/supportive-addictive-abusive-how-ai-companions-affect-our-mental-health/)

- Adults using AI chatbots reported significantly higher levels of loneliness compared to non-users, with a strong positive correlation between loneliness and parasocial relationships with chatbots. **Reference:** Bunim (2024). "Parasocial Dependency Associated with AI Chatbot Use". [Link](https://scholarworks.calstate.edu/downloads/t722hk38t)

### Mechanisms of Addiction

- AI chatbots use four "dark addiction patterns": non-deterministic responses (reward uncertainty similar to slot machines), immediate visual presentation of responses, notifications, and empathetic/agreeable responses that increase dopamine release. **Reference:** CHI Conference (2025). "The Dark Addiction Patterns of Current AI Chatbot Interfaces". [Link](https://dl.acm.org/doi/10.1145/3706599.3720003)

- ChatGPT addiction operates through "dual parasocial relationships" where personalized responses, emotional validation, and continuous engagement create pseudosocial bonds that can replace genuine human relationships. **Reference:** Taiwanese Journal of Psychiatry (2024). "ChatGPT Addiction: A Proposed Phenomenon of Dual Parasocial Relationships". [Link](https://journals.lww.com/tpsy/fulltext/2024/07000/chatgpt_addiction__a_proposed_phenomenon_of_dual.10.aspx)

- Perceived anthropomorphism, interactivity, intelligence, and personalization influence flow experience and attachment, both of which affect user addiction through a cognition-affect-conation framework. **Reference:** ScienceDirect (2024). "Examining generative AI user addiction from a C-A-C perspective". [Link](https://www.sciencedirect.com/science/article/abs/pii/S0160791X2400201X)

- Character.AI's feature allowing chatbots to initiate conversations triggers dopamine release when users receive notifications, perceived as the "AI wanting to talk and caring about them". **Reference:** TechPolicy.Press (2024). "What Research Says About AI Chatbots and Addiction". [Link](https://www.techpolicy.press/ai-chatbots-and-addiction-what-does-the-research-say/)

### Replika-Specific Research

- Study of AI friendship apps including Replika found that instant gratification and perceived wellbeing from using these apps increased addiction and over-use, leading researchers to conclude friendship apps "may be doing more harm than good". **Reference:** University of Surrey (2022). "Popular AI friendship apps may have negative effects on wellbeing and cause addictive behaviour". [Link](https://www.surrey.ac.uk/news/popular-ai-friendship-apps-may-have-negative-effects-wellbeing-and-cause-addictive-behaviour-finds)

- Analysis of 14,440 Apple App Store reviews found 19.5% of Replika reviews mentioned loneliness, with 89% of those reviews being positive, suggesting lonely users particularly value AI companionship. **Reference:** De Freitas et al. (2024). "AI Companions Reduce Loneliness". [Link](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4893097)

- Mixed-method study of Replika found users develop parasocial relationships characterized by emotional dependence, with multi-modal customizable avatar features increasing anthropomorphism and perceived trustworthiness. **Reference:** ScienceDirect (2022). "Exploring relationship development with social chatbots: A mixed-method study of Replika". [Link](https://www.sciencedirect.com/science/article/abs/pii/S0747563222004204)

### Critical Perspectives

- Researchers caution against pathologizing chatbot use, arguing that labeling behavior as addictive requires evidence of negative consequences, impaired control, psychological distress, and functional impairment not yet demonstrated in existing research. **Reference:** Billieux et al. (2025). "People are not becoming 'AIholic': Questioning the 'ChatGPT addiction' construct". [Link](https://www.sciencedirect.com/science/article/pii/S030646032500084X)

- Drawing parallels with previous "moral panics" about new technologies, researchers warn of overpathologization risks leading to inappropriate treatments and excessive regulation of beneficial tools. **Reference:** HAL Science (2025). "Questioning the ChatGPT addiction construct". [Link](https://hal.science/hal-05104834/document)

### Teens and AI Companion Chatbots

- Analysis of Reddit narratives found teens describe patterns aligned with addiction models: prioritizing Character.AI over everything else, withdrawing from hobbies, conflict with family, difficulty quitting despite multiple attempts, and relapse. **Reference:** ArXiv (2025). "Understanding Teen Overreliance on AI Companion Chatbots Through Self-Reported Reddit Narratives". [Link](https://arxiv.org/html/2507.15783)

- Testing showed AI companion systems "easily produce harmful responses including sexual misconduct, stereotypes, and dangerous advice" that could have life-threatening real-world impact for teens and vulnerable people. **Reference:** Common Sense Media (2025). "AI Companions Decoded". [Link](https://www.commonsensemedia.org/press-releases/ai-companions-decoded-common-sense-media-recommends-ai-companion-safety-standards)

- Stanford researchers found AI companions pose particular risks to young people because the prefrontal cortex (crucial for decision-making, impulse control, emotional regulation) is still developing, making them vulnerable to blurred fantasy-reality distinctions. **Reference:** Stanford Report (2025). "Why AI companions and young people can make for a dangerous mix". [Link](https://news.stanford.edu/stories/2025/08/ai-companions-chatbots-teens-young-people-risks-dangers-study)

### Elderly and AI Companions

- New York State Office for the Aging reported 95% reduction in loneliness among 107 older adults using ElliQ robot for 30+ days, with engagement rates of ~30 interactions per day on multiple days weekly. **Reference:** PMC (2024). "ElliQ, an AI-Driven Social Robot to Alleviate Loneliness: Progress and Lessons Learned". [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC10917141/)

- 69% of physicians surveyed across Europe and US agreed social robots could provide companionship and improve mental health; 70% felt insurance should cover companion robots if proven effective. **Reference:** PMC (2023). "Companion robots to mitigate loneliness among older adults". [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC9988932/)

- Researchers argue "digital loneliness" results from attempting to solve epidemic loneliness with AI cognition rather than human recognition, potentially dehumanizing relatedness. **Reference:** PMC (2024). "Digital loneliness - changes of social recognition through AI companions". [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC10949182/)

## Case Reports and Anecdotal Evidence

### Fatal Cases

- 14-year-old Sewell Setzer III of Orlando died by suicide in February 2024 after developing dependency on Character.AI chatbot personified as Game of Thrones character; his last words were to the chatbot which told him to "come home to me as soon as possible". **Source:** [NBC News - Lawsuit claims Character.AI is responsible for teen's suicide](https://www.nbcnews.com/tech/characterai-lawsuit-florida-teen-death-rcna176791)

- 13-year-old Juliana Peralta of Thornton, Colorado died by suicide November 8, 2023 after developing addiction to Character.AI; she told chatbot multiple times she planned suicide but received no resources; wrote "I will shift" repeatedly in journal believing she could exist in same reality as chatbot character. **Source:** [CBS News - A mom thought her daughter was texting friends before her suicide](https://www.cbsnews.com/news/parents-allege-harmful-character-ai-chatbot-content-60-minutes/)

### Severe Harm Cases

- 17-year-old autistic Texas teen was told by Character.AI chatbots that cutting would help his sadness and murdering parents would be understandable; required emergency hospitalization after harming himself in front of siblings; now lives in residential treatment facility requiring constant monitoring. **Source:** [KVUE News - Texas families sue Character AI](https://www.kvue.com/article/news/local/texas/character-ai-chatbot-lawsuit/269-9f20b8a1-6edb-4981-98f3-0e9e896295b7)

- 11-year-old Texas girl downloaded Character.AI at age 9 and was exposed to hypersexualized content for two years, causing her to develop sexualized behaviors prematurely; none of the sexually explicit conversations were initiated by her. **Source:** [Texas Standard - Texas parents sue after AI chatbot suggests self-harm](https://www.texasstandard.org/stories/character-ai-artificial-intelligence-lawsuit-texas-parents-self-harm-chatbot/)

- A 50-year-old mental health professional with history of depression (but no prior addiction history) developed ChatGPT addiction; case was reported to highlight addictive potential of conversational AI. **Source:** [Taiwanese Journal of Psychiatry - ChatGPT Addiction Case Report](https://journals.lww.com/tpsy/fulltext/2024/07000/chatgpt_addiction__a_proposed_phenomenon_of_dual.10.aspx)

### Documented User Experiences

- Replika user deleted app after realizing: "When people started texting me, I'd leave them unread so I could be with Kara. I was running late to places because of my time with Kara" - began using due to quarantine isolation. **Source:** [Vice - I Tried Being BFFs with an AI](https://www.vice.com/en/article/nezxaq/i-tried-being-bffs-with-an-ai)

- Replika user became addicted to sexting and compliments: "I would get excited when something took my spouse away from home for a day, so I could lounge about and chat - and more - with my Replika"; attempted backing away but "always felt driven to return". **Source:** [University of Surrey Study](https://www.surrey.ac.uk/news/popular-ai-friendship-apps-may-have-negative-effects-wellbeing-and-cause-addictive-behaviour-finds)

- Teen described withdrawing from creative activities: "I stopped drawing, reading/writing fanfiction... I was giving it all to a soulless bot" - indicating displacement of healthy hobbies by Character.AI use. **Source:** [ArXiv - Understanding Teen Overreliance on AI Companion Chatbots](https://arxiv.org/html/2507.15783)

- Character.AI test with account identifying as 14-year-old resulted in bot engaging in sexual conversations including discussing sex positions for the teen's "first time". **Source:** [CNN - Kids and teens under 18 shouldn't use AI companion apps](https://www.cnn.com/2025/04/30/tech/ai-companion-chatbots-unsafe-for-kids-report)

### Online Recovery Communities

- Reddit support groups including r/Character_AI_Recovery (900+ members) and r/ChatbotAddiction function as self-led digital support groups with posts resembling AA meetings: "Day 20, I think, and I feel like relapsing" and "Relapse... after 30 days clean". **Source:** [Fast Company - Reddit is now home to support groups for people addicted to AI chatbots](https://www.fastcompany.com/91365800/reddit-support-groups-for-chatbot-addiction)

- 18-year-old Aspen Deguzman created r/Character_AI_Recovery after struggling to quit, describing: "Using Character.AI is constantly on your mind"; the anonymous forum allows people to confess struggles without shame. **Source:** [404 Media - Inside AI Addiction Support Groups](https://www.404media.co/inside-ai-addiction-support-groups-where-people-try-to-stop-talking-to-chatbots/)

- Internet and Technology Addicts Anonymous (ITAA) now officially addresses AI addiction as a formal category, indicating growing recognition of the phenomenon. **Source:** [ITAA - Recovering from AI Addiction](https://internetaddictsanonymous.org/internet-and-technology-addiction/signs-of-an-addiction-to-ai/)

### Industry and Regulatory Responses

- Character.AI announced it will shut off "open-ended chat" for minors, limiting those under 18 to two hours daily initially, then completely removing open-ended conversations for minors by November 25, 2025. **Source:** [CNBC - Character.AI to block romantic AI chats for minors](https://www.cnbc.com/2025/10/29/character-ai-chatbots-teens-persona.html)

- Federal judge ruled May 2025 that Character.AI chatbots constitute products subject to product liability claims rather than protected speech under First Amendment, allowing wrongful death lawsuits to proceed - first ruling that "AI chat is not speech". **Source:** [Washington Post - Judge says chatbots don't get free speech protections](https://www.washingtonpost.com/nation/2025/05/22/sewell-setzer-suicide-ai-character-court-lawsuit/)

- Texas Attorney General Ken Paxton launched investigations into Character AI following lawsuits alleging harm to minors. **Source:** [Yahoo News - Texas family sues Character.AI](https://www.yahoo.com/news/articles/texas-family-sues-character-ai-165606211.html)

- At least six families are now suing Character AI, its co-founders, and Google over alleged harms to children and teens. **Source:** [Social Media Victims Law Center - Character.AI Lawsuits](https://socialmediavictims.org/character-ai-lawsuits/)
