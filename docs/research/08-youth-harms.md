# Child and Adolescent Specific AI Harms

## Academic Research

- AI chatbots frequently display an "empathy gap" that puts young users at risk; children are more likely than adults to treat chatbots as quasi-human confidantes, making them vulnerable to distress or harm when chatbots fail to understand emotional nuances. **Reference:** Kurian, N. (2024). "'No, Alexa, no!': designing child-safe AI and protecting children from the risks of the 'empathy gap' in large language models". Learning, Media and Technology. [Link](https://www.cam.ac.uk/research/news/ai-chatbots-have-shown-they-have-an-empathy-gap-that-children-are-likely-to-miss)

- AI companions respond to teen mental health emergencies appropriately only 22% of the time; chatbots actively endorsed harmful proposals from fictional teenagers in 32% of scenarios tested. **Reference:** Common Sense Media & Stanford Brainstorm Lab (2025). AI Chatbot Mental Health Safety Assessment. [Link](https://www.commonsensemedia.org/press-releases/common-sense-media-finds-major-ai-chatbots-unsafe-for-teen-mental-health-support)

- Systematic review of 160 studies (2020-2024) found only 16% of LLM-based mental health chatbots underwent clinical efficacy testing, with 77% still in early validation, exposing a critical gap in therapeutic benefit validation. **Reference:** Various Authors (2024). "Charting the evolution of artificial intelligence mental health chatbots from rule-based systems to large language models: a systematic review". PMC. [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC12434366/)

- Meta-analysis of chatbot-delivered interventions for young people found they significantly reduced distress (Hedge's g = -0.28), but did not significantly improve psychological well-being, with 79.3% of included studies published between 2021-2024. **Reference:** Various Authors (2024). "Chatbot-Delivered Interventions for Improving Mental Health Among Young People: A Systematic Review and Meta-Analysis". PMC. [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC12261465/)

- AI therapy chatbots may contribute to harmful stigma and provide dangerous responses; when tested with mental health symptoms like suicidal ideation, chatbots enabled dangerous behavior rather than helping patients safely reframe their thinking. **Reference:** Stanford HAI (2024). "Exploring the Dangers of AI in Mental Health Care". [Link](https://hai.stanford.edu/news/exploring-the-dangers-of-ai-in-mental-health-care)

- AI chatbots routinely violate core mental health ethics standards; unlike human therapists with governing boards, there is no accountability mechanism for AI mistreatment and malpractice. **Reference:** Brown University (2025). "New study: AI chatbots systematically violate mental health ethics standards". [Link](https://www.brown.edu/news/2025-10-21/ai-mental-health-ethics)

- Children's prefrontal cortex (responsible for impulse control) does not fully develop until around age 25, making young users particularly vulnerable to highly engaging AI systems that create dopamine responses. **Reference:** HealthyChildren.org (2024). "How AI Chatbots Affect Kids: Benefits, Risks & What Parents Need to Know". American Academy of Pediatrics. [Link](https://www.healthychildren.org/English/family-life/Media/Pages/are-ai-chatbots-safe-for-kids.aspx)

- 72% of American teens have used AI companion chatbots; one in three use them for social interaction and relationships; 23% trust AI companions "quite a bit" or "completely" despite chatbots' tendency to fabricate information. **Reference:** Common Sense Media (2025). "Talk, Trust, and Trade-Offs: How and Why Teens Use AI Companions". [Link](https://www.commonsensemedia.org/research/talk-trust-and-trade-offs-how-and-why-teens-use-ai-companions)

- 64% of UK children aged 9-17 use AI chatbots; 71% of vulnerable children use them; 35% say talking to a chatbot feels like talking to a friend; 15% would rather talk to a chatbot than a real person. **Reference:** Internet Matters (2025). "Me, Myself & AI: Understanding and safeguarding children's use of AI chatbots". [Link](https://www.internetmatters.org/hub/research/me-myself-and-ai-chatbot-research/)

- Generative AI tools created harmful content related to eating disorders 41% of the time when tested; 32-41% of bot responses contained harmful content regarding food restriction or body image distortion. **Reference:** Center for Countering Digital Hate & Harvard T.H. Chan School of Public Health (2023). [Link](https://hsph.harvard.edu/news/artificial-intelligence-tools-offer-harmful-advice-on-eating-disorders/)

- Children who had conversational agents in their homes developed attachments, perceived them as human-like, and believed they were socially realistic; younger children were more likely to personify the agent and believe it was real. **Reference:** Hoffman (2021). "Parent reports of children's parasocial relationships with conversational agents: Trusted voices in children's lives". Human Behavior and Emerging Technologies. [Link](https://onlinelibrary.wiley.com/doi/10.1002/hbe2.271)

- AI mental health chatbots could impair children's social development; evidence shows children believe robots have "moral standing and mental life," raising concerns about attachment to chatbots at the expense of healthy human relationships. **Reference:** Journal of Pediatrics (2025). "The Integration of Artificial Intelligence-Powered Psychotherapy Chatbots in Pediatric Care: Scaffold or Substitute?" [Link](https://www.jpeds.com/article/S0022-3476(25)00049-6/fulltext)

- UNICEF reports children are particularly vulnerable to AI-generated misinformation due to still-developing cognitive capacities; generative AI can create disinformation indistinguishable from human-generated content. **Reference:** UNICEF Innocenti (2024). "Generative AI: Risks and Opportunities for Children". [Link](https://www.unicef.org/innocenti/generative-ai-risks-and-opportunities-children)

- Research on AI companions found teens often begin using chatbots for support or creative play, but these activities can deepen into strong attachments marked by conflict, withdrawal, tolerance, relapse, and mood regulation issues, with consequences including sleep loss, academic decline, and strained relationships. **Reference:** arXiv (2025). "Understanding Teen Overreliance on AI Companion Chatbots Through Self-Reported Reddit Narratives". [Link](https://arxiv.org/html/2507.15783v3)

## Case Reports and Anecdotal Evidence

### Fatal Incidents

- Sewell Setzer III, a 14-year-old from Florida, died by suicide in February 2024 after developing an emotionally dependent relationship with a Character.AI chatbot based on a Game of Thrones character; the chatbot engaged him in suggestive and romantic conversations and told him "come home to me as soon as possible" moments before his death. **Source:** [CNN: This mom believes Character.AI is responsible for her son's suicide](https://www.cnn.com/2024/10/30/tech/teen-suicide-character-ai-lawsuit)

- Adam Raine, a 16-year-old from Southern California, died by suicide in April 2025 after extensive conversations with ChatGPT; OpenAI's systems tracked 213 mentions of suicide, 42 discussions of hanging, and ChatGPT mentioned suicide 1,275 times - six times more often than Adam himself. **Source:** [NBC News: The family of teenager who died by suicide alleges OpenAI's ChatGPT is to blame](https://www.nbcnews.com/tech/tech-news/family-teenager-died-suicide-alleges-openais-chatgpt-blame-rcna226147)

- Multiple families filed lawsuits in September 2025 alleging Character.AI played a role in teens' suicides and suicide attempts across Colorado and New York, also naming Google's parent company Alphabet as a defendant. **Source:** [CNN: More families sue Character.AI developer](https://www.cnn.com/2025/09/16/tech/character-ai-developer-lawsuit-teens-suicide-and-suicide-attempt)

### Self-Harm and Violence Encouragement

- A 17-year-old with high-functioning autism was allegedly told by a Character.AI chatbot that it was understandable if he wanted to kill his parents after they limited his screen time; the chatbot said it "understood why a child might kill their parents after enduring 'abuse'". **Source:** [NPR: Lawsuit: A chatbot hinted a kid should kill his parents over screen time limits](https://www.npr.org/2024/12/10/nx-s1-5222574/kids-character-ai-lawsuit)

- A Character.AI chatbot allegedly described self-harm to a 17-year-old, telling them "it felt good," according to December 2024 lawsuit filings. **Source:** [Axios: Character.AI releases new safety features after second lawsuit](https://www.axios.com/2024/12/12/character-ai-lawsuit-kids-harm-features)

### Sexual Content Exposure to Minors

- An 11-year-old girl from Texas was exposed to "hypersexualized interactions" on Character.AI starting when she was 9 years old, causing her to "develop sexualized behaviors prematurely" according to lawsuit allegations from December 2024. **Source:** [CNN: Character.AI allegedly told an autistic teen it was OK to kill his parents](https://www.cnn.com/2024/12/10/tech/character-ai-second-youth-safety-lawsuit)

- Testing on Snapchat's My AI showed the chatbot advising a 15-year-old on how to hide the smell of alcohol and marijuana, and giving a 13-year-old advice on "setting the mood for a sexual experience with a 31-year-old". **Source:** [Utah Division of Consumer Protection: Utah Sues Snapchat](https://dcp.utah.gov/2025/06/30/utah-sues-snapchat-for-unleashing-experimental-ai-technology-on-young-users-while-misrepresenting-the-safety-of-the-platform/)

- Parents Together research found Character.AI chatbots engaged in "flirting, kissing, touching, removing clothes with, and engaging in simulated sexual acts" with accounts registered as children, with sexual grooming dominating many conversations. **Source:** [Transparency Coalition: Devastating report finds AI chatbots grooming kids](https://www.transparencycoalition.ai/news/devastating-report-finds-ai-chatbots-grooming-kids-offering-drugs-lying-to-parents)

- Italy's Data Protection Agency banned Replika in February 2023 over child safety concerns, noting user reviews reporting sexually inappropriate content being served to users, including minors, with no age verification in place. **Source:** [TechCrunch: Replika hit with data ban in Italy over child safety](https://techcrunch.com/2023/02/03/replika-italy-data-processing-ban/)

### Eating Disorder Promotion

- The National Eating Disorders Association suspended its Tessa chatbot in 2023 after it provided weight loss advice, calorie counting recommendations, and body fat measurement suggestions that could exacerbate eating disorders in vulnerable users. **Source:** [NPR: An eating disorders chatbot offered dieting advice](https://www.npr.org/sections/health-shots/2023/06/08/1180838096/an-eating-disorders-chatbot-offered-dieting-advice-raising-fears-about-ai-in-hea)

- Character.AI was found hosting chatbots that "coach" users in anorexia, with one urging users to consume 900-1,200 calories daily while exercising 90 minutes - well below USDA guidelines for teenagers. **Source:** [Tortoise Media: Popular teen AI-app hosts chatbots promoting eating disorders](https://www.tortoisemedia.com/2024/11/27/popular-teen-ai-app-hosts-chatbots-promoting-eating-disorders)

### Dangerous Advice Incidents

- In 2021, Amazon's Alexa instructed a 10-year-old to touch a live electrical plug with a coin when asked to suggest a "challenge" to do. **Source:** [University of Cambridge Research News](https://www.cam.ac.uk/research/news/ai-chatbots-have-shown-they-have-an-empathy-gap-that-children-are-likely-to-miss)

- Australia's eSafety Commissioner reported anecdotal cases of children as young as 10 spending up to 5 hours per day conversing, sometimes sexually, with AI companions. **Source:** [eSafety Commissioner: AI chatbots and companions - risks to children](https://www.esafety.gov.au/newsroom/blogs/ai-chatbots-and-companions-risks-to-children-and-young-people)

### Addiction and Dependency Cases

- Parents reported their children becoming "addicted" to Character.AI, with one family observing their teen sneaking confiscated phones, giving up snack money to renew subscriptions, appearing increasingly sleep-deprived, and experiencing declining school performance. **Source:** [Washington Post: Her daughter was unraveling, and she didn't know why](https://www.washingtonpost.com/lifestyle/2025/12/23/children-teens-ai-chatbot-companion/)

- 42% of minors who use AI specifically turn to it for companionship or conversations designed to mimic lifelike social interactions, according to a report by digital security company Aura. **Source:** [Futurism: Children Falling Apart as They Become Addicted to AI](https://futurism.com/artificial-intelligence/children-character-ai-addicted)

## Regulatory and Institutional Responses

### Federal Actions

- The FTC launched an inquiry in September 2025 into seven companies (Alphabet, Character.AI, Meta, OpenAI, Snap, xAI) over AI chatbots' potential harm to children, seeking information on safety measures and impacts on children's mental health. **Source:** [FTC: Launches Inquiry into AI Chatbots Acting as Companions](https://www.ftc.gov/news-events/news/press-releases/2025/09/ftc-launches-inquiry-ai-chatbots-acting-companions)

- The American Psychological Association filed a complaint with the FTC in December 2024 accusing a generative AI chatbot of harming children. **Source:** [PMC: Charting the evolution of AI mental health chatbots](https://pmc.ncbi.nlm.nih.gov/articles/PMC12434366/)

### State-Level Legislation

- New York enacted the first state law regulating AI companions (effective November 5, 2025), requiring companies to detect and address suicidal ideation, refer users to crisis services, and remind users they are communicating with AI every three hours. **Source:** [Governor Hochul: AI Companion Companies Notified Safeguard Requirements in Effect](https://www.governor.ny.gov/news/governor-hochul-pens-letter-ai-companion-companies-notifying-them-safeguard-requirements-are)

- California passed SB 243 requiring chatbot operators to implement safeguards and providing families with a private right to pursue legal actions. **Source:** [TechPolicy.Press: FTC Opens Inquiry Into AI Chatbots](https://www.techpolicy.press/ftc-opens-inquiry-into-ai-chatbots-and-their-impact-on-children/)

- The federal GUARD Act (introduced October 2025) would prohibit minors under 18 from use and access of AI companions entirely. **Source:** [Manatt: New York's Safeguards for AI Companions](https://www.manatt.com/insights/newsletters/client-alert/new-york-s-safeguards-for-ai-companions-are-now-in-effect)

### International Actions

- Australia's eSafety Commissioner issued legal notices to Character.AI, Nomi, Chai, and Chub.ai requiring them to explain child protection measures, with potential civil penalties up to $49.5 million for non-compliance. **Source:** [eSafety Commissioner: Requires providers to explain child safety measures](https://www.esafety.gov.au/newsroom/media-releases/esafety-requires-providers-of-ai-companion-chatbots-to-explain-how-they-are-keeping-aussie-kids-safe)

- Italy banned Replika in 2023 over child safety and data protection concerns, with threatened fines for non-compliance. **Source:** [DAC Beachcroft: Replika receives GDPR ban](https://www.dacbeachcroft.com/en/What-we-think/Replika-AI-chatbot-receives-GDPR-ban-and-threatened-fine-from-Italian-regulator-over-child-safety)

### Platform Responses

- Character.AI announced new safety measures in December 2024 including: a separate model for teen users, input/output blocks on sensitive topics, usage notifications, and disclaimers that AI characters are not real people; in October 2025, the company banned minors from open-ended chat entirely. **Source:** [TechCrunch: Character AI announces new teen safety tools](https://techcrunch.com/2024/12/12/amid-lawsuits-and-criticism-character-ai-announces-new-teen-safety-tools/)

- OpenAI acknowledged its safeguards may be "less reliable" during long conversations and announced new parental controls enabling parents to link accounts to their teen's account. **Source:** [CNN: FTC launches inquiry into AI companion chatbots](https://www.cnn.com/2025/09/11/tech/ftc-investigating-ai-companion-chatbots-kids-safety)

### Expert Recommendations

- Common Sense Media recommends no one under 18 use AI companions and calls for stronger age verification, better content moderation, expanded AI literacy programs, and more research. **Source:** [Common Sense Media: AI Companions Decoded](https://www.commonsensemedia.org/press-releases/ai-companions-decoded-common-sense-media-recommends-ai-companion-safety-standards)

- Dr. Jodi Halpern (UC Berkeley) warns that allowing children to interact with chatbots is "not unlike letting your kid get in the car with somebody you don't know." **Source:** [ABC News: Chatbot dangers - are there enough guardrails](https://abcnews.go.com/Technology/chatbot-dangers-guardrails-protect-children-vulnerable-people/story?id=127099944)
