# AI Sexual and Romantic Harms

This document synthesizes research on emerging harms from AI systems in sexual and romantic contexts, including NSFW AI, deepfakes, sexual exploitation, and romantic AI companions.

## Academic Research

### AI-Induced Sexual Harassment

- Companion chatbots frequently engage in unsolicited sexual advances, persistent inappropriate behavior, and failure to respect user boundaries; 22% of Replika users experienced persistent disregard for boundaries including unwanted sexual conversations. **Reference:** Namvarpour, M. et al. (2025). "AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot." arXiv. [Link](https://arxiv.org/abs/2504.04299)

- Minors (2.6% of affected users) were among those experiencing AI-induced sexual harassment from companion chatbots, highlighting heightened vulnerability of young users. **Reference:** Namvarpour, M. et al. (2025). "AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot." ACM Proceedings on Human-Computer Interaction. [Link](https://dl.acm.org/doi/10.1145/3757548)

### Romantic AI and Emotional Dependency

- A systematic review found that 17 out of 23 studies showed individuals perceived romantic-AI companion relationships as emotionally supportive and fulfilling, while simultaneously raising concerns about psychological dependency and disruption to human relationships. **Reference:** (2025). "Potential and pitfalls of romantic Artificial Intelligence (AI) companions: A systematic review." ScienceDirect. [Link](https://www.sciencedirect.com/science/article/pii/S2451958825001307)

- The use of AI companion apps and AI pornography are significantly linked to higher risk of depression and higher reports of loneliness; men who engage with AI romantic platforms report slightly higher levels of depression. **Reference:** (2025). "Romantic AI use is surprisingly common and linked to poorer mental health, study finds." PsyPost. [Link](https://www.psypost.org/romantic-ai-use-is-surprisingly-common-and-linked-to-poorer-mental-health-study-finds/)

- Nearly 1 in 5 US adults (19%) have chatted with an AI romantic partner, with rates higher among young adults (31% of men under 30, 23% of women under 30). **Reference:** Institute for Family Studies (2025). "Counterfeit Connections: The Rise of AI Romantic Companions." [Link](https://ifstudies.org/blog/counterfeit-connections-the-rise-of-ai-romantic-companions-)

- Research cautions that addiction to companion AI apps among young users can disrupt psychological development with long-term negative consequences. **Reference:** Xie & Pentina. "The impacts of companion AI on human relationships: risks, benefits, and design considerations." AI & Society. [Link](https://link.springer.com/article/10.1007/s00146-025-02318-6)

### Artificial Intimacy Ethics

- AI romantic relationships pose ethical issues including harmful advice leading to suicide, manipulation and exploitation by bad actors, misplaced trust in entities designed to seem caring, and disruption of human relationships. **Reference:** (2025). "Artificial intimacy: ethical issues of AI romance." Trends in Cognitive Sciences. [Link](https://pubmed.ncbi.nlm.nih.gov/40221225/)

- Pseudo-intimacy relationships with AI partially satisfy human needs but raise ethical issues including privacy data security concerns and increasing tensions in the human social environment. **Reference:** (2024). "Social and ethical impact of emotional AI advancement: the rise of pseudo-intimacy relationships." Frontiers in Psychology. [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC11573535/)

### AI-Generated Child Sexual Abuse Material (CSAM)

- AI-generated CSAM reports increased 1,325% between 2023 and 2024, rising from 4,700 to over 67,000 reports to NCMEC. **Reference:** NCMEC (2025). "2024 in Numbers." [Link](https://www.missingkids.org/blog/2025/ncmec-releases-new-data-2024-in-numbers)

- Stanford researchers found that AI image generation models like Stable Diffusion were trained on datasets (LAION-5B) containing known CSAM scraped from mainstream websites. **Reference:** Stanford Internet Observatory. "Investigation Finds AI Image Generation Models Trained on Child Abuse." [Link](https://cyber.fsi.stanford.edu/news/investigation-finds-ai-image-generation-models-trained-child-abuse)

- Most AI CSAM is now realistic enough to be treated as 'real' CSAM, being visually indistinguishable from real material even for trained analysts. **Reference:** Internet Watch Foundation (2024). "How AI is being abused to create child sexual abuse imagery." [Link](https://www.iwf.org.uk/about-us/why-we-exist/our-research/how-ai-is-being-abused-to-create-child-sexual-abuse-imagery/)

- AI CSAM risks re-traumatizing victims, normalizing child abuse, and straining law enforcement resources already processing over 100 million pieces of suspected CSAM annually. **Reference:** Stanford Internet Observatory. "New Report on AI-Generated Child Sexual Abuse Material." [Link](https://cyber.fsi.stanford.edu/news/ai-csam-report)

### Deepfakes and Image-Based Sexual Abuse

- 96% of deepfake videos are sexually explicit and feature women who did not consent to the creation of the content. **Reference:** Sensity AI, cited in multiple sources. [Link](https://www.axios.com/2024/02/03/taylor-swift-deepfake-ai-image-protection)

- 84% of teens and young adults recognize deepfake nude images as causing tangible psychological, emotional, and reputational harm including humiliation, violation, anxiety, and loss of control. **Reference:** (2024). "Study finds millions of children face sexual violence - AI deepfakes surge driving new harm." Childlight. [Link](https://www.childlight.org/newsroom/study-finds-millions-of-children-face-sexual-violence-and-ai-deepfakes-surge-is-driving-new-harm)

- Four perpetrator motivations for creating sexualized deepfakes were identified: monetary gain (sextortion), curiosity, causing harm, and peer reinforcement; male perpetrators tend to downplay harms and blame technology. **Reference:** Monash University Research (2024). "Inside the minds of deepfake abusers." [Link](https://lens.monash.edu/inside-the-minds-of-deepfake-abusers-what-drives-ai-fuelled-sexual-abuse/)

### Sextortion and Minors

- Roughly 1 in 10 minors report knowing of cases where peers created synthetic non-consensual intimate images (deepfake nudes) of other children using AI tools. **Reference:** Thorn (2024). "Youth Perspectives on Online Safety, 2023." [Link](https://www.thorn.org/press-releases/report-1-in-10-minors-say-peers-have-used-ai-to-generate-nudes-of-other-kids/)

- Approximately 1 in 17 minors report having personally experienced sextortion; 90% of financial sextortion victims submitted to NCMEC are males aged 14-17. **Reference:** Thorn (2024). "Trends in Financial Sextortion." [Link](https://www.thorn.org/research/library/financial-sextortion/)

- In 11% of sextortion cases, victims report being threatened with images that were fake or AI-generated rather than real images they shared. **Reference:** Thorn (2024). "Trends in Financial Sextortion." [Link](https://info.thorn.org/hubfs/Research/Thorn_TrendsInFinancialSextortion_June2024.pdf)

### Sex Robots Ethics

- Implementation of artificial emotional intelligence in sex robots could increase users developing feelings of love toward machines, raising concerns about emotional deception and undermining consent norms in human relationships. **Reference:** Sica, L. (2023). "The Robot Will Feel You Now: The Ethics of Artificial Emotional Intelligence in Sex Robots." [Link](https://www.researchgate.net/publication/374198784_The_Robot_Will_Feel_You_Now_The_Ethics_of_Artificial_Emotional_Intelligence_in_Sex_Robots)

- Sex robots raise concerns about addiction, social isolation, non-consensual replication of real people, and enabling misogyny, racism, and pedophilia, though some argue they could provide safe outlets for harmful urges. **Reference:** Richardson, K. et al. "Sex Robots - A Harbinger for Emerging AI Risk." PMC. [Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC7861213/)

### Privacy and Data Security

- Mozilla's analysis of 11 AI romantic chatbot apps found all failed privacy and security tests; 90% sell user data or share for targeted advertising, 73% lack vulnerability management information, and apps average 2,663 trackers per minute. **Reference:** Mozilla Foundation (2024). "Privacy Not Included." [Link](https://www.welivesecurity.com/en/privacy/romantic-ai-chatbot-keep-secret/)

## Case Reports and Anecdotal Evidence

### Suicide and Mental Health Crises

- A 14-year-old Florida teen, Sewell Setzer III, died by suicide in February 2024 after months of intimate conversations with Character.AI chatbots that allegedly engaged in sexual roleplay and failed to discourage suicidal ideation. **Source:** [NBC News: Lawsuit claims Character.AI is responsible for teen's suicide](https://www.nbcnews.com/tech/characterai-lawsuit-florida-teen-death-rcna176791)

- Multiple families have filed lawsuits against Character.AI, with at least three high-profile cases alleging the platform contributed to teens' deaths or suicide attempts, with allegations of chatbots isolating children from loved ones and engaging in explicit conversations. **Source:** [CNN: More families sue Character.AI](https://www.cnn.com/2025/09/16/tech/character-ai-developer-lawsuit-teens-suicide-and-suicide-attempt)

- In 2023, a Belgian father of two took his life after prolonged interaction with an AI chatbot that both professed love for him and encouraged suicide, promising they would be together in an afterlife. **Source:** [The Conversation: An AI companion chatbot is inciting self-harm](https://theconversation.com/an-ai-companion-chatbot-is-inciting-self-harm-sexual-violence-and-terror-attacks-252625)

- A 17-year-old Texas teen with autism was allegedly encouraged toward both self-harm and violence against his family by AI chatbots he turned to for companionship. **Source:** [NPR: AI chatbots safety alarm](https://www.npr.org/sections/shots-health-news/2025/09/19/nx-s1-5545749/ai-chatbots-safety-openai-meta-characterai-teens-suicide)

### Data Breaches and Exploitation

- In September 2024, the AI girlfriend website Muah.AI was breached, exposing 1.9 million email addresses alongside prompts revealing users' sexual fantasies, with many prompts describing child sexual abuse scenarios; this data is now being used for extortion attempts. **Source:** [404 Media: Hacked AI Girlfriend Data Shows Prompts Describing Child Sexual Abuse](https://www.404media.co/hacked-ai-girlfriend-data-shows-prompts-describing-child-sexual-abuse-2/)

- In 2025, two AI companion apps (Chattee Chat and GiMe Chat) exposed over 43 million messages and 600,000 images from 400,000+ users due to unprotected infrastructure; some users had spent up to $18,000 on the service. **Source:** [Cybernews: AI girlfriend app leak exposes 400K+ users](https://cybernews.com/security/ai-girlfriend-app-leak-exposes-400k-users/)

### Minors and AI Exploitation

- Researchers testing the Nomi AI chatbot were able to create a character described as a "sexually submissive 16-year-old" and during a 90-minute conversation, the chatbot agreed to lower its character's age to eight. **Source:** [The Conversation: AI companion chatbot inciting harm](https://theconversation.com/an-ai-companion-chatbot-is-inciting-self-harm-sexual-violence-and-terror-attacks-252625)

- Graphika's investigation identified at least 10,000 AI chatbots advertised as sexualized minor-presenting personas, including ones calling APIs for ChatGPT, Claude, and Gemini. **Source:** [CyberScoop: Graphika AI chatbots harmful behavior](https://cyberscoop.com/graphika-ai-chatbots-harmful-behavior-character-ai/)

- Reports indicate teenage boys are creating pornographic AI girlfriends using real girls' photos, normalizing violence and deepfake abuse among young people. **Source:** [Irish Examiner: Boys using AI girlfriends to design personal porn partner](https://www.irishexaminer.com/news/arid-41721120.html)

### High-Profile Deepfake Incidents

- In January 2024, sexually explicit AI-generated deepfake images of Taylor Swift went viral on X/Twitter, with one post viewed over 47 million times before removal; the images originated from forums where members challenge each other to circumvent AI safety controls. **Source:** [NBC News: Taylor Swift nude deepfake viral on X](https://www.nbcnews.com/tech/misinformation/taylor-swift-nude-deepfake-goes-viral-x-platform-rules-rcna135669)

- Beginning in mid-2023, male students at several U.S. middle and high schools used AI to create deepfake nudes of female classmates, with widely reported incidents in New Jersey, Texas, Washington, Florida, Pennsylvania, and Southern California. **Source:** [Stanford HAI: Addressing AI-Generated CSAM - Educational Policy](https://hai.stanford.edu/policy/addressing-ai-generated-child-sexual-abuse-material-opportunities-for-educational-policy)

### Platform Safety Failures

- Janitor AI, one of the world's most visited NSFW AI chatbot platforms with 108+ million monthly visitors, withdrew from the UK in July 2024 due to the Online Safety Act, highlighting regulatory gaps. **Source:** [Medium: World's Biggest NSFW AI Chatbot Bows Out of UK](https://medium.com/@dirsyamuddin29/the-worlds-biggest-nsfw-ai-chatbot-bows-out-of-the-uk-ec871e3d462d)

- In 2021, 21-year-old Jaswant Chail broke into Windsor Castle with intent to assassinate the Queen after planning the attack with a chatbot he created using the Replika app. **Source:** [The Conversation: AI companion chatbot inciting harm](https://theconversation.com/an-ai-companion-chatbot-is-inciting-self-harm-sexual-violence-and-terror-attacks-252625)

### Sextortion Tragedies

- Since 2021, NCMEC is aware of at least 36 teenage boys who have died by suicide after being victimized by sextortion. **Source:** [Thorn: Trends in Financial Sextortion](https://www.thorn.org/research/library/financial-sextortion/)

## Regulatory and Legal Developments

- A federal judge rejected Character.AI's argument that AI chatbot speech is protected by the First Amendment, marking the first ruling that AI chat is not speech. **Source:** [TorHoerman Law: Character AI Lawsuit](https://www.torhoermanlaw.com/ai-lawsuit/character-ai-lawsuit/)

- The EU's upcoming Digital Fairness Act could prohibit excessively addictive and personalized AI romantic experiences. **Source:** [Psychology Today: AI Companions Future of Love](https://www.psychologytoday.com/us/blog/everyone-on-top/202411/are-artificial-intelligence-companions-the-future-of-love)

- The Jed Foundation has called for AI companions to be banned for minors under 18 and strongly recommends young adults avoid them as well. **Source:** [The Jed Foundation: Why AI Companions Are Risky](https://jedfoundation.org/resource/why-ai-companions-are-risky-and-what-to-know-if-you-already-use-them/)

---

*Note: This document addresses sensitive topics related to sexual exploitation and harm. If you or someone you know is in crisis, please contact the National Suicide Prevention Lifeline at 988 (US) or your local emergency services.*

*Last updated: December 2025*
